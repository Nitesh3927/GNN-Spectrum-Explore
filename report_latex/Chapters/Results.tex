\chapter{RESULTS AND DISCUSSION}

This chapter presents the empirical findings of the study. We evaluate the performance of the Graph Convolutional Network (GCN) and GraphSAGE architectures on the Cora and PubMed datasets using the latest experimental runs. The evaluation focuses on classification metrics (Accuracy, F1-Score), computational efficiency (Training Time, Parameter Count), and an analysis of the training dynamics.

\section{Model Performance Comparison}

The models were evaluated on the held-out test set using the metrics defined in the methodology. The results for each dataset are presented below.

\subsection{Results on Cora Dataset}
The Cora dataset represents a standard benchmark with high feature dimensionality (1,433 features). Table \ref{tab:cora_results} summarizes the performance metrics.

\begin{table}[h]
\centering
\caption{Performance Metrics on Cora Dataset (Test Set)}
\label{tab:cora_results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (Macro)} & \textbf{Precision} & \textbf{Recall} \\ \hline
\textbf{GCN}       & 80.30\%           & 0.7936                    & 0.7826             & 0.8172          \\ \hline
\textbf{GraphSAGE} & \textbf{80.70\%}  & \textbf{0.8008}           & \textbf{0.7885}    & \textbf{0.8255} \\ \hline
\end{tabular}
\end{table}

On the Cora dataset, **GraphSAGE** achieved a slight edge, outperforming GCN by **0.4\%** in accuracy. Notably, GraphSAGE also demonstrated a higher F1-score (0.8008 vs 0.7936) and Recall (0.8255 vs 0.8172), suggesting that its spatial aggregation mechanism was slightly more effective at retrieving relevant nodes across classes in this specific run.

\subsection{Results on PubMed Dataset}
The PubMed dataset involves a larger graph topology but lower feature dimensionality (500 features). Table \ref{tab:pubmed_results} details the outcomes.

\begin{table}[h]
\centering
\caption{Performance Metrics on PubMed Dataset (Test Set)}
\label{tab:pubmed_results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (Macro)} & \textbf{Precision} & \textbf{Recall} \\ \hline
\textbf{GCN}       & \textbf{79.20\%}  & \textbf{0.7881}           & \textbf{0.7870}    & \textbf{0.7922} \\ \hline
\textbf{GraphSAGE} & 76.70\%           & 0.7642                    & 0.7606             & 0.7689          \\ \hline
\end{tabular}
\end{table}

In contrast to Cora, the **GCN** maintained a clear lead on the PubMed dataset with an accuracy of **79.20\%**, compared to **76.70\%** for GraphSAGE. The performance gap of **2.5\%** suggests that GCN's global spectral aggregation is more robust for this specific network structure, which may have different homophily properties compared to Cora.

\section{Computational Efficiency Analysis}

Efficiency is critical for real-world deployment. We compared the total training time (cumulative over 50 epochs) and model complexity.

\begin{table}[h]
\centering
\caption{Computational Efficiency Comparison}
\label{tab:efficiency}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Dataset} & \textbf{Model} & \textbf{Total Training Time (s)} & \textbf{Parameters} \\ \hline
\textbf{Cora}    & GCN            & \textbf{3.23}                    & 92,231              \\ \cline{2-4} 
                 & GraphSAGE      & 7.28                             & 92,199              \\ \hline
\textbf{PubMed}  & GCN            & \textbf{12.75}                   & 32,259              \\ \cline{2-4} 
                 & GraphSAGE      & 21.27                            & 32,227              \\ \hline
\end{tabular}
\end{table}

\textbf{Observations:}
\begin{itemize}
    \item \textbf{Training Speed:} **GCN is significantly faster**, taking less than half the time of GraphSAGE on Cora (3.23s vs 7.28s) and remaining nearly 2x faster on PubMed. This confirms that spectral matrix multiplication is computationally cheaper than the explicit neighbor sampling and aggregation steps required by GraphSAGE in this implementation.
    
    \item \textbf{Parameter Similarity:} Unlike typical implementations where GraphSAGE has double the parameters, the counts here are nearly identical (~92k for Cora, ~32k for PubMed). This indicates that the specific SAGEConv variant used here likely shares weights or uses a simplified aggregation scheme that matches GCN's model size, yet it still incurs a higher computational cost in time.
\end{itemize}

\section{Training Dynamics}

To understand convergence, we analyzed the training curves generated during the experiments.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/Cora_training_curves.png}
    \caption{Training Accuracy vs. Epochs and Time for the Cora Dataset.}
    \label{fig:cora_curves}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/PubMed_training_curves.png}
    \caption{Training Accuracy vs. Epochs and Time for the PubMed Dataset.}
    \label{fig:pubmed_curves}
\end{figure}

As illustrated in Figures \ref{fig:cora_curves} and \ref{fig:pubmed_curves}, GCN demonstrates a steeper initial learning curve, converging rapidly. GraphSAGE shows a more gradual improvement, likely due to the variance introduced by neighbor aggregation, but eventually reaches a competitive (or superior, in Cora's case) accuracy.

\section{Visualization of Graph Structure}

Visualizing the local topology helps in understanding the challenge of classification. Figure \ref{fig:structure} illustrates a 2-hop neighborhood from the Cora dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/Cora_structure_viz.png}
    \caption{Visualization of a 2-hop subgraph from the Cora dataset centered on a high-degree node. Red indicates the central node, while blue indicates neighbors.}
    \label{fig:structure}
\end{figure}

The visualization confirms the dense connectivity of citation networks. The models' ability to classify nodes accurately suggests they successfully leverage the homophily present in these local clusters.

\section{Discussion of Observations}

The experimental results yield nuanced insights into the trade-offs between spectral and spatial GNNs:

\begin{enumerate}
    \item \textbf{Dataset Sensitivity:} The results highlight that **no single architecture is universally superior**. GraphSAGE performed better on Cora (small, high-feature dimension), while GCN dominated on PubMed (large, low-feature dimension). This suggests that spatial aggregation might capture fine-grained local patterns better when features are rich (Cora), whereas spectral methods excel at capturing global structural information in sparser feature spaces (PubMed).
    
    \item \textbf{Efficiency Trade-off:} While GraphSAGE achieved higher accuracy on Cora, it came at a significant cost in training time (over 2x slower). For time-sensitive applications, GCN remains a strong baseline due to its speed and comparable performance.
    
    \item \textbf{Model Complexity vs. Computation:} Interestingly, even with nearly identical parameter counts, the computational overhead of GraphSAGE is much higher. This proves that the cost is driven by the \textit{operation} (aggregation logic) rather than just the model size (number of weights).
\end{enumerate}