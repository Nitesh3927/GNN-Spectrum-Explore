% \chapter{RESULTS AND DISCUSSION}

% This chapter presents the empirical findings of the study. We evaluate the performance of the Graph Convolutional Network (GCN) and GraphSAGE architectures on the Cora and PubMed datasets. The evaluation focuses on classification metrics (Accuracy, F1-Score), computational efficiency (Training Time, Parameter Count), and an analysis of the training dynamics.

% \section{Model Performance Comparison}

% The models were evaluated on the held-out test set using the metrics defined in the methodology. The results for each dataset are presented below.

% \subsection{Results on Cora Dataset}
% The Cora dataset represents a standard benchmark with a high feature dimensionality (1,433 features). Table \ref{tab:cora_results} summarizes the performance metrics.

% \begin{table}[h]
% \centering
% \caption{Performance Metrics on Cora Dataset (Test Set)}
% \label{tab:cora_results}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (Macro)} & \textbf{Precision} & \textbf{Recall} \\ \hline
% \textbf{GCN}       & 79.30\%           & 0.7811                    & 0.7819             & 0.7831          \\ \hline
% \textbf{GraphSAGE} & 78.10\%           & 0.7672                    & 0.7635             & 0.7758          \\ \hline
% \end{tabular}
% \end{table}

% On the Cora dataset, the **GCN** achieved a slightly superior performance, outperforming GraphSAGE by approximately **1.2\%** in accuracy. The higher F1-score and Precision indicate that the GCN was better at distinguishing between the seven subject classes, particularly in a transductive setting where the graph structure is static.

% \subsection{Results on PubMed Dataset}
% The PubMed dataset is significantly larger in terms of nodes but has lower feature dimensionality (500 features). Table \ref{tab:pubmed_results} details the outcomes.

% \begin{table}[h]
% \centering
% \caption{Performance Metrics on PubMed Dataset (Test Set)}
% \label{tab:pubmed_results}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (Macro)} & \textbf{Precision} & \textbf{Recall} \\ \hline
% \textbf{GCN}       & 79.10\%           & 0.7761                    & 0.7755             & 0.7770          \\ \hline
% \textbf{GraphSAGE} & 77.40\%           & 0.7634                    & 0.7588             & 0.7712          \\ \hline
% \end{tabular}
% \end{table}

% Similar to the results on Cora, the **GCN** maintained a lead on the PubMed dataset with an accuracy of **79.10\%**, compared to **77.40\%** for GraphSAGE. The consistent performance gap suggests that the spectral aggregation of GCN is highly effective for citation networks where homophily is strong.

% \section{Computational Efficiency Analysis}

% Beyond predictive performance, the efficiency of GNNs is critical for real-world deployment. We compared the total training time (over 50 epochs) and the model complexity (number of parameters).

% \begin{table}[h]
% \centering
% \caption{Computational Efficiency Comparison}
% \label{tab:efficiency}
% \begin{tabular}{|l|l|c|c|}
% \hline
% \textbf{Dataset} & \textbf{Model} & \textbf{Total Training Time (s)} & \textbf{Parameters} \\ \hline
% \multirow{2}{*}{\textbf{Cora}}   & GCN        & 0.4004                           & 92,359              \\ \cline{2-4} 
%                                  & GraphSAGE  & \textbf{0.3546}                  & 92,359              \\ \hline
% \multirow{2}{*}{\textbf{PubMed}} & GCN        & 0.3639                           & 32,451              \\ \cline{2-4} 
%                                  & GraphSAGE  & \textbf{0.3621}                  & 32,451              \\ \hline
% \end{tabular}
% \end{table}

% \textbf{Observations:}
% \begin{itemize}
%     \item \textbf{Training Speed:} GraphSAGE demonstrated a speed advantage, particularly on the Cora dataset where it was approximately \textbf{11\% faster} than GCN. This validates the efficiency of spatial sampling over full-graph spectral operations. On PubMed, the times were comparable, likely due to the highly optimized sparse matrix operations in PyTorch Geometric.
%     \item \textbf{Model Complexity:} Both models possess an identical number of trainable parameters for a given dataset (92k for Cora, 32k for PubMed). The large difference in parameters between datasets is due to the input feature dimension ($1433$ for Cora vs $500$ for PubMed), highlighting that the first GNN layer is the most memory-intensive component.
% \end{itemize}

% \section{Training Dynamics}

% To understand the convergence behavior of the models, we analyzed the training curves generated during the experiments.

% \begin{figure}[h]
%     \centering
%     % Placeholder for the image generated by your code
%     \includegraphics[width=0.9\textwidth]{imgs/Cora_training_curves.png}
%     \caption{Training Accuracy vs. Epochs and Time for the Cora Dataset.}
%     \label{fig:cora_curves}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     % Placeholder for the image generated by your code
%     \includegraphics[width=0.9\textwidth]{imgs/PubMed_training_curves.png}
%     \caption{Training Accuracy vs. Epochs and Time for the PubMed Dataset.}
%     \label{fig:pubmed_curves}
% \end{figure}

% As illustrated in Figures \ref{fig:cora_curves} and \ref{fig:pubmed_curves}, both models converge rapidly within the first 20 epochs. The GCN curves generally appear smoother and reach a higher asymptotic accuracy plateau. GraphSAGE exhibits slightly more variance in the early epochs, which is characteristic of sampling-based aggregation methods where the neighborhood definition can vary slightly.

% \section{Visualization of Graph Structure}

% Visualizing the local topology helps in understanding the challenge of classification. Figure \ref{fig:structure} illustrates a 2-hop neighborhood from the Cora dataset.

% \begin{figure}[h]
%     \centering
%     % Placeholder for the image generated by your code
%     \includegraphics[width=0.7\textwidth]{imgs/Cora_structure_viz.png}
%     \caption{Visualization of a 2-hop subgraph from the Cora dataset centered on a high-degree node. Red indicates the central node, while blue indicates neighbors.}
%     \label{fig:structure}
% \end{figure}

% The visualization confirms the dense connectivity of citation networks. The success of both models indicates their ability to effectively propagate features across these dense clusters, leveraging the homophily (similarity) of connected papers.

% \section{Discussion of Observations}

% The experimental results yield several key insights into the behavior of spectral versus spatial GNNs:

% \begin{enumerate}
%     \item \textbf{Superiority of GCN in Transductive Settings:} The GCN consistently outperformed GraphSAGE in terms of accuracy. This is expected in a transductive setting (where all nodes are present during training). The GCN utilizes the full graph Laplacian, allowing it to capture global structural properties more effectively than the localized aggregation of GraphSAGE.
    
%     \item \textbf{Efficiency of Spatial Methods:} Despite slightly lower accuracy, GraphSAGE proved to be computationally efficient. Its mean-aggregation mechanism is less computationally expensive than the matrix multiplications required by GCN, making it a strong candidate for scenarios where latency is a priority.
    
%     \item \textbf{Impact of Feature Dimensionality:} The drastic difference in parameter counts between Cora (92k) and PubMed (32k) highlights that GNN complexity is heavily driven by node feature size. For datasets with extremely high-dimensional features (e.g., raw text), dimensionality reduction might be necessary before applying GNNs to prevent memory bottlenecks.
% \end{enumerate}



% \chapter{RESULTS AND DISCUSSION}

% This chapter presents the empirical findings of the study. We evaluate the performance of the Graph Convolutional Network (GCN) and GraphSAGE architectures on the Cora and PubMed datasets. The evaluation focuses on classification metrics (Accuracy, F1-Score), computational efficiency (Training Time, Parameter Count), and an analysis of the training dynamics.

% \section{Model Performance Comparison}

% The models were evaluated on the held-out test set using the metrics defined in the methodology. The results for each dataset are presented below.

% \subsection{Results on Cora Dataset}
% The Cora dataset represents a standard benchmark with a high feature dimensionality (1,433 features). Table \ref{tab:cora_results} summarizes the performance metrics.

% \begin{table}[h]
% \centering
% \caption{Performance Metrics on Cora Dataset (Test Set)}
% \label{tab:cora_results}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (Macro)} & \textbf{Precision} & \textbf{Recall} \\ \hline
% \textbf{GCN}       & \textbf{80.50\%}  & \textbf{0.7976}           & \textbf{0.7828}    & \textbf{0.8221} \\ \hline
% \textbf{GraphSAGE} & 79.30\%           & 0.7862                    & 0.7757             & 0.8026          \\ \hline
% \end{tabular}
% \end{table}

% On the Cora dataset, the **GCN** achieved superior performance, outperforming GraphSAGE by **1.2\%** in accuracy. The higher Recall (0.8221) indicates that GCN was particularly effective at retrieving relevant nodes across the seven subject classes.

% \subsection{Results on PubMed Dataset}
% The PubMed dataset is significantly larger in terms of nodes but has lower feature dimensionality (500 features). Table \ref{tab:pubmed_results} details the outcomes.

% \begin{table}[h]
% \centering
% \caption{Performance Metrics on PubMed Dataset (Test Set)}
% \label{tab:pubmed_results}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (Macro)} & \textbf{Precision} & \textbf{Recall} \\ \hline
% \textbf{GCN}       & \textbf{78.80\%}  & \textbf{0.7869}           & \textbf{0.7843}    & \textbf{0.7911} \\ \hline
% \textbf{GraphSAGE} & 76.00\%           & 0.7542                    & 0.7498             & 0.7601          \\ \hline
% \end{tabular}
% \end{table}

% The trend continues on the PubMed dataset, where **GCN** maintained a lead with an accuracy of **78.80\%**, compared to **76.00\%** for GraphSAGE. The performance gap widens on this larger graph, suggesting GCN's spectral aggregation is highly robust for transductive citation networks.

% \section{Computational Efficiency Analysis}

% Efficiency is critical for deployment. We compared the total training time (cumulative over 50 epochs) and model complexity.

% \begin{table}[h]
% \centering
% \caption{Computational Efficiency Comparison}
% \label{tab:efficiency}
% \begin{tabular}{|l|l|c|c|}
% \hline
% \textbf{Dataset} & \textbf{Model} & \textbf{Total Training Time (s)} & \textbf{Parameters} \\ \hline
% \multirow{2}{*}{\textbf{Cora}}   & GCN        & \textbf{0.9741}                  & \textbf{92,231}     \\ \cline{2-4} 
%                                  & GraphSAGE  & 2.4283                           & 184,391             \\ \hline
% \multirow{2}{*}{\textbf{PubMed}} & GCN        & \textbf{3.7954}                  & \textbf{32,259}     \\ \cline{2-4} 
%                                  & GraphSAGE  & 7.7313                           & 64,451              \\ \hline
% \end{tabular}
% \end{table}

% \textbf{Observations:}
% \begin{itemize}
%     \item \textbf{Parameter Efficiency:} The **GCN is significantly more parameter-efficient**, requiring roughly half the parameters of GraphSAGE (e.g., 92k vs. 184k on Cora). This is because the standard `SAGEConv` layer learns two separate weight matrices—one for the node itself and one for the aggregated neighborhood—whereas `GCNConv` learns a single weight matrix applied to the renormalized adjacency computation.
    
%     \item \textbf{Training Speed:} In this full-batch training setup, **GCN was faster** (approx. 2.5x faster on Cora). GraphSAGE's explicit aggregation and concatenation operations, combined with the larger parameter set, resulted in higher computational overhead per epoch compared to the efficient sparse matrix multiplication used by GCN.
% \end{itemize}

% \section{Training Dynamics}

% To understand convergence, we analyzed the training curves generated during the experiments.

% \begin{figure}[h]
%     \centering
%     % Placeholder for image
%     \includegraphics[width=0.9\textwidth]{imgs/Cora_training_curves.png}
%     \caption{Training Accuracy vs. Epochs and Time for the Cora Dataset.}
%     \label{fig:cora_curves}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     % Placeholder for image
%     \includegraphics[width=0.9\textwidth]{imgs/PubMed_training_curves.png}
%     \caption{Training Accuracy vs. Epochs and Time for the PubMed Dataset.}
%     \label{fig:pubmed_curves}
% \end{figure}

% As illustrated in Figures \ref{fig:cora_curves} and \ref{fig:pubmed_curves}, both models converge rapidly. However, GraphSAGE's higher complexity requires slightly more time to reach peak performance, as evidenced by the "Accuracy vs. Time" plots.

% \section{Visualization of Graph Structure}

% Visualizing the local topology helps in understanding the challenge of classification. Figure \ref{fig:structure} illustrates a 2-hop neighborhood from the Cora dataset.

% \begin{figure}[h]
%     \centering
%     % Placeholder for image
%     \includegraphics[width=0.7\textwidth]{imgs/Cora_structure_viz.png}
%     \caption{Visualization of a 2-hop subgraph from the Cora dataset centered on a high-degree node. Red indicates the central node, while blue indicates neighbors.}
%     \label{fig:structure}
% \end{figure}

% The visualization confirms the dense connectivity of citation networks. The success of both models indicates their ability to effectively propagate features across these dense clusters, leveraging the homophily of connected papers.

% \section{Discussion of Observations}

% The experimental results yield important insights into the trade-offs between spectral and spatial GNNs in a \textbf{transductive, full-batch} setting:

% \begin{enumerate}
%     \item \textbf{GCN Dominance in Transductive Tasks:} GCN consistently outperformed GraphSAGE in accuracy and efficiency. In settings where the entire graph is available during training (transductive), the global spectral approximation of GCN is highly effective and computationally cheaper than the localized aggregation of GraphSAGE.
    
%     \item \textbf{Cost of Expressivity:} GraphSAGE's design (separate self/neighbor weights) makes it potentially more expressive and capable of inductive learning (generalizing to unseen nodes). However, this comes at the cost of doubling the parameter count and increasing training time, which did not translate to higher accuracy on these specific static benchmarks.
    
%     \item \textbf{Feature Dimensionality Impact:} The parameter count drops significantly for PubMed (32k) compared to Cora (92k) for GCN, despite PubMed having more nodes. This confirms that the input feature dimension (1433 for Cora vs. 500 for PubMed) is the primary driver of model size in shallow GNNs, rather than the number of nodes or edges.
% \end{enumerate}


\chapter{RESULTS AND DISCUSSION}

This chapter presents the empirical findings of the study. We evaluate the performance of the Graph Convolutional Network (GCN) and GraphSAGE architectures on the Cora and PubMed datasets using the latest experimental runs. The evaluation focuses on classification metrics (Accuracy, F1-Score), computational efficiency (Training Time, Parameter Count), and an analysis of the training dynamics.

\section{Model Performance Comparison}

The models were evaluated on the held-out test set using the metrics defined in the methodology. The results for each dataset are presented below.

\subsection{Results on Cora Dataset}
The Cora dataset represents a standard benchmark with high feature dimensionality (1,433 features). Table \ref{tab:cora_results} summarizes the performance metrics.

\begin{table}[h]
\centering
\caption{Performance Metrics on Cora Dataset (Test Set)}
\label{tab:cora_results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (Macro)} & \textbf{Precision} & \textbf{Recall} \\ \hline
\textbf{GCN}       & 80.30\%           & 0.7936                    & 0.7826             & 0.8172          \\ \hline
\textbf{GraphSAGE} & \textbf{80.70\%}  & \textbf{0.8008}           & \textbf{0.7885}    & \textbf{0.8255} \\ \hline
\end{tabular}
\end{table}

On the Cora dataset, **GraphSAGE** achieved a slight edge, outperforming GCN by **0.4\%** in accuracy. Notably, GraphSAGE also demonstrated a higher F1-score (0.8008 vs 0.7936) and Recall (0.8255 vs 0.8172), suggesting that its spatial aggregation mechanism was slightly more effective at retrieving relevant nodes across classes in this specific run.

\subsection{Results on PubMed Dataset}
The PubMed dataset involves a larger graph topology but lower feature dimensionality (500 features). Table \ref{tab:pubmed_results} details the outcomes.

\begin{table}[h]
\centering
\caption{Performance Metrics on PubMed Dataset (Test Set)}
\label{tab:pubmed_results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (Macro)} & \textbf{Precision} & \textbf{Recall} \\ \hline
\textbf{GCN}       & \textbf{79.20\%}  & \textbf{0.7881}           & \textbf{0.7870}    & \textbf{0.7922} \\ \hline
\textbf{GraphSAGE} & 76.70\%           & 0.7642                    & 0.7606             & 0.7689          \\ \hline
\end{tabular}
\end{table}

In contrast to Cora, the **GCN** maintained a clear lead on the PubMed dataset with an accuracy of **79.20\%**, compared to **76.70\%** for GraphSAGE. The performance gap of **2.5\%** suggests that GCN's global spectral aggregation is more robust for this specific network structure, which may have different homophily properties compared to Cora.

\section{Computational Efficiency Analysis}

Efficiency is critical for real-world deployment. We compared the total training time (cumulative over 50 epochs) and model complexity.

\begin{table}[h]
\centering
\caption{Computational Efficiency Comparison}
\label{tab:efficiency}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Dataset} & \textbf{Model} & \textbf{Total Training Time (s)} & \textbf{Parameters} \\ \hline
\textbf{Cora}    & GCN            & \textbf{3.23}                    & 92,231              \\ \cline{2-4} 
                 & GraphSAGE      & 7.28                             & 92,199              \\ \hline
\textbf{PubMed}  & GCN            & \textbf{12.75}                   & 32,259              \\ \cline{2-4} 
                 & GraphSAGE      & 21.27                            & 32,227              \\ \hline
\end{tabular}
\end{table}

\textbf{Observations:}
\begin{itemize}
    \item \textbf{Training Speed:} **GCN is significantly faster**, taking less than half the time of GraphSAGE on Cora (3.23s vs 7.28s) and remaining nearly 2x faster on PubMed. This confirms that spectral matrix multiplication is computationally cheaper than the explicit neighbor sampling and aggregation steps required by GraphSAGE in this implementation.
    
    \item \textbf{Parameter Similarity:} Unlike typical implementations where GraphSAGE has double the parameters, the counts here are nearly identical (~92k for Cora, ~32k for PubMed). This indicates that the specific SAGEConv variant used here likely shares weights or uses a simplified aggregation scheme that matches GCN's model size, yet it still incurs a higher computational cost in time.
\end{itemize}

\section{Training Dynamics}

To understand convergence, we analyzed the training curves generated during the experiments.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/Cora_training_curves.png}
    \caption{Training Accuracy vs. Epochs and Time for the Cora Dataset.}
    \label{fig:cora_curves}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/PubMed_training_curves.png}
    \caption{Training Accuracy vs. Epochs and Time for the PubMed Dataset.}
    \label{fig:pubmed_curves}
\end{figure}

As illustrated in Figures \ref{fig:cora_curves} and \ref{fig:pubmed_curves}, GCN demonstrates a steeper initial learning curve, converging rapidly. GraphSAGE shows a more gradual improvement, likely due to the variance introduced by neighbor aggregation, but eventually reaches a competitive (or superior, in Cora's case) accuracy.

\section{Visualization of Graph Structure}

Visualizing the local topology helps in understanding the challenge of classification. Figure \ref{fig:structure} illustrates a 2-hop neighborhood from the Cora dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/Cora_structure_viz.png}
    \caption{Visualization of a 2-hop subgraph from the Cora dataset centered on a high-degree node. Red indicates the central node, while blue indicates neighbors.}
    \label{fig:structure}
\end{figure}

The visualization confirms the dense connectivity of citation networks. The models' ability to classify nodes accurately suggests they successfully leverage the homophily present in these local clusters.

\section{Discussion of Observations}

The experimental results yield nuanced insights into the trade-offs between spectral and spatial GNNs:

\begin{enumerate}
    \item \textbf{Dataset Sensitivity:} The results highlight that **no single architecture is universally superior**. GraphSAGE performed better on Cora (small, high-feature dimension), while GCN dominated on PubMed (large, low-feature dimension). This suggests that spatial aggregation might capture fine-grained local patterns better when features are rich (Cora), whereas spectral methods excel at capturing global structural information in sparser feature spaces (PubMed).
    
    \item \textbf{Efficiency Trade-off:} While GraphSAGE achieved higher accuracy on Cora, it came at a significant cost in training time (over 2x slower). For time-sensitive applications, GCN remains a strong baseline due to its speed and comparable performance.
    
    \item \textbf{Model Complexity vs. Computation:} Interestingly, even with nearly identical parameter counts, the computational overhead of GraphSAGE is much higher. This proves that the cost is driven by the \textit{operation} (aggregation logic) rather than just the model size (number of weights).
\end{enumerate}